{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/main/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from rich import print\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "# data_path = pathlib.Path(\"/devcode/GATE-private/notebooks/gate-results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def convert_to_datetime(date_string):\n",
    "    # convert the string to a datetime object\n",
    "    date_object = datetime.strptime(date_string, \"%Y-%m-%dT%H:%M:%S\")\n",
    "    return date_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8469/8469 [01:34<00:00, 90.02it/s] \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "api = wandb.Api()\n",
    "\n",
    "runs = api.runs(\"machinelearningbrewery/gate-exp-0-8-6\")\n",
    "exp_name_to_time_dict = {}\n",
    "metric_keys = [\n",
    "    \"testing/ensemble_3/text_to_image_accuracy-epoch-mean\",\n",
    "    \"testing/ensemble_3/text_to_image_accuracy_top_5-epoch-mean\",\n",
    "    \"testing/ensemble_3/image_to_text_accuracy-epoch-mean\",\n",
    "    \"testing/ensemble_3/image_to_text_accuracy_top_5-epoch-mean\",\n",
    "    \"testing/ensemble_3/bs-macro\",\n",
    "    \"testing/ensemble_3/aps-macro\",\n",
    "    \"testing/ensemble_3/auc-macro\",\n",
    "    \"testing/ensemble_3/accuracy_top_1-epoch-mean\",\n",
    "    \"testing/ensemble_3/accuracy_top_5-epoch-mean\",\n",
    "]\n",
    "all_keys = set()\n",
    "summary_list, config_list, name_list = [], [], []\n",
    "for run in tqdm(runs):\n",
    "    # .summary contains the output keys/values for metrics like accuracy.\n",
    "    #  We call ._json_dict to omit large files\n",
    "    metric_dict = run.summary._json_dict\n",
    "    timestamp = convert_to_datetime(run.heartbeatAt)\n",
    "\n",
    "    # .config contains the hyperparameters.\n",
    "    #  We remove special values that start with _.\n",
    "    config = {k: v for k, v in run.config.items() if not k.startswith(\"_\")}\n",
    "    if \"exp_name\" in config:\n",
    "        exp_name = config[\"exp_name\"]\n",
    "        if exp_name in exp_name_to_time_dict:\n",
    "            if timestamp > exp_name_to_time_dict[exp_name]:\n",
    "                exp_name_to_time_dict[exp_name] = timestamp\n",
    "            else:\n",
    "                continue\n",
    "        # .name is the human-readable name of the run.\n",
    "        if \"hades\" in exp_name:\n",
    "            summary_list.append(metric_dict)\n",
    "            config_list.append(config)\n",
    "            name_list.append(exp_name)\n",
    "\n",
    "            for key in summary_list[-1].keys():\n",
    "                if \"testing\" in key and (\"macro\" in key or \"mean\" in key):\n",
    "                    all_keys.add(key)\n",
    "\n",
    "runs_df = pd.DataFrame(\n",
    "    {\"summary\": summary_list, \"config\": config_list, \"name\": name_list}\n",
    ")\n",
    "# print(all_keys)\n",
    "# print(runs_df)\n",
    "# runs_df.to_csv(\"project.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_summary_list = []\n",
    "new_config_list = []\n",
    "new_name_list = []\n",
    "\n",
    "for summary, config, name in zip(summary_list, config_list, name_list):\n",
    "    if name.endswith(\"-7\"):\n",
    "        new_summary_list.append(summary)\n",
    "        new_config_list.append(config)\n",
    "        new_name_list.append(name)\n",
    "\n",
    "summary_list = new_summary_list\n",
    "config_list = new_config_list\n",
    "name_list = new_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'testing/ensemble_3/text_to_image_accuracy-epoch-mean'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'testing/ensemble_3/bs-macro'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'testing/ensemble_3/aps-macro'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'testing/ensemble_3/image_to_text_accuracy-epoch-mean'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'testing/ensemble_3/text_to_image_accuracy_top_5-epoch-mean'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'testing/ensemble_3/accuracy_top_1-epoch-mean'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'testing/ensemble_3/auc-macro'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'testing/ensemble_3/accuracy_top_5-epoch-mean'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'testing/ensemble_3/image_to_text_accuracy_top_5-epoch-mean'</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'testing/ensemble_3/text_to_image_accuracy-epoch-mean'\u001b[0m,\n",
       "    \u001b[32m'testing/ensemble_3/bs-macro'\u001b[0m,\n",
       "    \u001b[32m'testing/ensemble_3/aps-macro'\u001b[0m,\n",
       "    \u001b[32m'testing/ensemble_3/image_to_text_accuracy-epoch-mean'\u001b[0m,\n",
       "    \u001b[32m'testing/ensemble_3/text_to_image_accuracy_top_5-epoch-mean'\u001b[0m,\n",
       "    \u001b[32m'testing/ensemble_3/accuracy_top_1-epoch-mean'\u001b[0m,\n",
       "    \u001b[32m'testing/ensemble_3/auc-macro'\u001b[0m,\n",
       "    \u001b[32m'testing/ensemble_3/accuracy_top_5-epoch-mean'\u001b[0m,\n",
       "    \u001b[32m'testing/ensemble_3/image_to_text_accuracy_top_5-epoch-mean'\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "selected_keys = set()\n",
    "for key in sorted(all_keys):\n",
    "    if (\n",
    "        \"loss\" not in key\n",
    "        and \"shape\" not in key\n",
    "        and \"colour\" not in key\n",
    "        and \"ensemble_1\" not in key\n",
    "        and \"logits\" not in key\n",
    "        and \"count\" not in key\n",
    "        and \"material\" not in key\n",
    "        and \"yes_no\" not in key\n",
    "        and \"size\" not in key\n",
    "        and \"similarities\" not in key\n",
    "    ):\n",
    "        selected_keys.add(key)\n",
    "print(selected_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "exp_dict = defaultdict(dict)\n",
    "\n",
    "for name, config, metric_dict in zip(name_list, config_list, summary_list):\n",
    "    if \"tali\" not in name and \"wit\" not in name:\n",
    "        if any([key in metric_dict.keys() for key in selected_keys]):\n",
    "            for key in selected_keys:\n",
    "                if key in metric_dict:\n",
    "                    exp_dict[name][key] = metric_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import csv\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def aggregate_experiments(\n",
    "    experiments: Dict[str, Dict[str, float]]\n",
    ") -> Dict[str, Dict[str, List[float]]]:\n",
    "    aggregated = collections.defaultdict(lambda: collections.defaultdict(list))\n",
    "\n",
    "    for experiment_name, metrics in experiments.items():\n",
    "        base_name = experiment_name.rsplit(\"-\", 1)[0]\n",
    "        for metric, value in metrics.items():\n",
    "            aggregated[base_name][metric].append(value)\n",
    "\n",
    "    return aggregated\n",
    "\n",
    "\n",
    "def create_csv(\n",
    "    output_filename: str,\n",
    "    aggregated_experiments: Dict[str, Dict[str, List[float]]],\n",
    ") -> None:\n",
    "    unique_metrics = set()\n",
    "    for _, metrics in aggregated_experiments.items():\n",
    "        unique_metrics.update(metrics.keys())\n",
    "\n",
    "    rows = []\n",
    "    for experiment_name, metrics in aggregated_experiments.items():\n",
    "        experiment_parts = experiment_name.split(\"-\", 2)\n",
    "        experiment_series, dataset_name = experiment_parts[:2]\n",
    "\n",
    "        row = {\n",
    "            \"Experiment-series\": experiment_series,\n",
    "            \"Dataset-name\": dataset_name,\n",
    "            \"Experiment-name\": experiment_name,\n",
    "            \"count\": 0,\n",
    "        }\n",
    "        for metric in unique_metrics:\n",
    "            values = metrics.get(metric, [])\n",
    "            count = len(values)\n",
    "            if \"NaN\" in values:\n",
    "                continue\n",
    "            mean_value = sum(values) / count if count > 0 else None\n",
    "            row[metric] = mean_value\n",
    "            row[\"count\"] = max(row[\"count\"], count)\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    with open(output_filename, \"w\", newline=\"\") as csvfile:\n",
    "        fieldnames = [\n",
    "            \"Experiment-series\",\n",
    "            \"Dataset-name\",\n",
    "            \"Experiment-name\",\n",
    "            \"count\",\n",
    "        ] + sorted(unique_metrics)\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "        for row in rows:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "aggregated_experiments = aggregate_experiments(exp_dict)\n",
    "\n",
    "# print(list(aggregated_experiments.keys()))\n",
    "create_csv(\"experiments_summary.csv\", aggregated_experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def prettify_metric(metric: str) -> str:\n",
    "    pretty_dict = {\n",
    "        \"accuracy_top_5\": \"acc@5\",\n",
    "        \"accuracy_top_1\": \"acc@1\",\n",
    "        \"text_to_image_accuracy\": \"txt2img acc\",\n",
    "        \"text_to_image_accuracy_top_5\": \"txt2img acc@5\",\n",
    "        \"image_to_text_accuracy\": \"img2txt acc\",\n",
    "        \"image_to_text_accuracy_top_5\": \"img2txt acc@5\",\n",
    "        \"auc-macro\": \"auc-macro\",\n",
    "        \"bs-macro\": \"bs-macro\",\n",
    "        \"aps-macro\": \"aps-macro\",\n",
    "    }\n",
    "    for key, value in pretty_dict.items():\n",
    "        if key in metric:\n",
    "            return metric.replace(key, value)\n",
    "    return metric\n",
    "\n",
    "\n",
    "def generate_latex_tables(\n",
    "    aggregated_experiments: Dict[str, Dict[str, List[float]]]\n",
    ") -> Dict[str, Dict[str, List[str]]]:\n",
    "    dataset_tables = collections.defaultdict(\n",
    "        lambda: collections.defaultdict(list)\n",
    "    )\n",
    "\n",
    "    for experiment_name, metrics in aggregated_experiments.items():\n",
    "        experiment_parts = experiment_name.split(\"-\", 2)\n",
    "        experiment_series, dataset_name, new_experiment_name = experiment_parts\n",
    "\n",
    "        for metric, values in metrics.items():\n",
    "            pretty_metric = (\n",
    "                prettify_metric(metric)\n",
    "                .replace(\"testing/ensemble_3/\", \"\")\n",
    "                .replace(\"-epoch-mean\", \"\")\n",
    "                .replace(\"_\", \" \")\n",
    "            )\n",
    "            count = len(values)\n",
    "            if \"NaN\" in values:\n",
    "                continue\n",
    "            mean_value = sum(values) / count if count > 0 else None\n",
    "            std_dev = np.std(values) if count > 0 else None\n",
    "            dataset_tables[dataset_name][pretty_metric].append(\n",
    "                (new_experiment_name, mean_value, std_dev)\n",
    "            )\n",
    "\n",
    "    return dataset_tables\n",
    "\n",
    "\n",
    "def write_latex_tables_to_file(\n",
    "    dataset_tables: Dict[str, Dict[str, List[str]]], output_filename: str\n",
    ") -> None:\n",
    "    with open(output_filename, \"w\") as output_file:\n",
    "        for dataset_name, metrics_map in dataset_tables.items():\n",
    "            output_file.write(f\"% {dataset_name} dataset table\\n\")\n",
    "            output_file.write(\"\\\\begin{table}[htbp]\\n\")\n",
    "            output_file.write(\n",
    "                f\"\\\\caption{{Results for the {dataset_name} dataset}}\\n\"\n",
    "            )\n",
    "\n",
    "            header = \" & \".join([metric for metric in metrics_map.keys()])\n",
    "            output_file.write(\n",
    "                f\"\\\\begin{{tabular}}{{|l|{'|'.join('c' * (2 * len(metrics_map)))}|}}\\\\hline\\n\"\n",
    "            )\n",
    "            output_file.write(f\"Experiment & {header}\\\\\\\\\\n\")\n",
    "            output_file.write(\"\\\\hline\\n\")\n",
    "\n",
    "            experiments = collections.defaultdict(dict)\n",
    "            for metric, experiment_values in metrics_map.items():\n",
    "                for new_experiment_name, value, std_dev in experiment_values:\n",
    "                    if value is not None:\n",
    "                        if metric not in experiments[new_experiment_name]:\n",
    "                            experiments[new_experiment_name][metric] = []\n",
    "                        experiments[new_experiment_name][metric].append(\n",
    "                            (value, std_dev)\n",
    "                        )\n",
    "\n",
    "            for new_experiment_name in sorted(experiments.keys()):\n",
    "                metric_values = experiments[new_experiment_name]\n",
    "                row_data = [\n",
    "                    f\"{sum(value for value, _ in metric_values.get(metric, [])) / len(metric_values[metric]):.2f} ± {np.mean([std_dev for _, std_dev in metric_values.get(metric, [])]):.2f}\"\n",
    "                    if metric in metric_values\n",
    "                    else \"NA\"\n",
    "                    for metric in metrics_map.keys()\n",
    "                ]\n",
    "                row_data_str = \" & \".join(row_data)\n",
    "                output_file.write(\n",
    "                    f\"{new_experiment_name} & {row_data_str}\\\\\\\\\\n\"\n",
    "                )\n",
    "                output_file.write(\"\\\\hline\\n\")\n",
    "\n",
    "            output_file.write(\"\\\\end{tabular}\\n\")\n",
    "            output_file.write(\"\\\\end{table}\\n\\n\")\n",
    "\n",
    "\n",
    "dataset_tables = generate_latex_tables(aggregated_experiments)\n",
    "write_latex_tables_to_file(dataset_tables, \"latex_tables.tex\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
